{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPq2km/Smlpvx7Ln4g2nQmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaganFoundr/PyTorchNN/blob/main/MNIST_logistic_GPU/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LINEAR REGRESSION\n",
        "\n",
        "#importing all the important libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "#inputs (temperature, rainfall, humidity)\n",
        "inputs=np.array([[73,67,45],\n",
        "                 [91,88,64],\n",
        "                 [87,134,58],\n",
        "                 [102,43,37],\n",
        "                 [69,96,70]],dtype='float32')\n",
        "\n",
        "#targets (apples, oranges)\n",
        "targets=np.array([[56,70],\n",
        "                 [81,101],\n",
        "                 [119,133],\n",
        "                 [22,37],\n",
        "                 [103,119]],dtype='float32')\n",
        "\n",
        "#converting inputs and targets from numpy format to tensor format\n",
        "inputs=torch.from_numpy(inputs)\n",
        "targets=torch.from_numpy(targets)\n",
        "\n",
        "# defining random weights and biases\n",
        "weights=torch.randn(2,3,requires_grad=True)\n",
        "bias=torch.randn(2,requires_grad=True)\n",
        "\n",
        "#function for the prediction equation for the model\n",
        "def model(x):\n",
        "  return x @ weights.t()+bias\n",
        "\n",
        "#function for the loss\n",
        "def mse(x,y):\n",
        "  diff=x-y\n",
        "  return torch.sum(diff*diff)/diff.numel()\n",
        "\n",
        "#training loop with 1000 epochs\n",
        "for i in range(1000):\n",
        "\n",
        "  #predicting using the inputs\n",
        "  prediction=model(inputs)\n",
        "\n",
        "  # checking the loss of the prediction\n",
        "  loss=mse(prediction, targets)\n",
        "  print(loss)\n",
        "\n",
        "  # backpropogating to compute gradients and update the weights\n",
        "  loss.backward()\n",
        "\n",
        "  #updating the weights without tracking the gradient\n",
        "  with torch.no_grad():\n",
        "\n",
        "    #learning rate value\n",
        "    lr=0.00001\n",
        "\n",
        "    #updating weights\n",
        "    weights-=weights.grad*lr\n",
        "\n",
        "    #updating bias\n",
        "    bias-=bias.grad*lr\n",
        "\n",
        "    #emptying the gradients of weights and bias\n",
        "    weights.grad.zero_()\n",
        "    bias.grad.zero_()\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "CdEho-pTaSMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MORE COMPLEX LINEAR REGRESSION\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "inputs=np.array([[73,67,43],[91,88,64],[87,134,58],\n",
        "                 [102,43,37],[69,96,70],[73,67,43],\n",
        "                 [91,88,64],[87,134,58],[102,43,37],\n",
        "                 [69,96,70],[73,67,43],[91,88,64],\n",
        "                 [87,134,58],[102,43,37],[69,96,70]],dtype='float32')\n",
        "\n",
        "targets=np.array([[56,70],[81,101],[119,133],\n",
        "                 [22,37],[103,119],[56,70],\n",
        "                 [81,101],[119,133],[22,37],\n",
        "                 [103,119],[56,70],[81,101],\n",
        "                 [119,133],[22,73],[103,119]],dtype='float32')\n",
        "\n",
        "inputs=torch.tensor(inputs)\n",
        "targets=torch.tensor(targets)\n",
        "\n",
        "training_data=TensorDataset(inputs,targets)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "training_data=DataLoader(training_data, batch_size, shuffle=True )\n",
        "\n",
        "model=nn.Linear(3,2)\n",
        "\n",
        "list(model.parameters())\n",
        "\n",
        "loss_fn = F.mse_loss\n",
        "\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.00001)\n",
        "\n",
        "def train_function(nepochs, model, loss_fn, optimizer):\n",
        "  for epochs in range(nepochs):\n",
        "    for x,y in training_data:\n",
        "\n",
        "      prediction = model(x)\n",
        "\n",
        "      loss = loss_fn(prediction, y)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    if (epochs+1)%10==0:\n",
        "      print(f\"epochs: {epochs+1}/{nepochs} , loss: {loss.item()}\")\n",
        "\n",
        "train_function(1000, model, loss_fn, optimizer)\n",
        "\n",
        "prediction=model(inputs)\n",
        "\n",
        "print(prediction)\n",
        "\n",
        "print(targets)"
      ],
      "metadata": {
        "id": "ZJN2ACY-uplj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOGISTIC REGRESSION USING MNIST\n",
        "\n",
        "#importing libraries\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
        "\n",
        "#main dataset and testdata\n",
        "dataset=MNIST(download=True, train=True, root=\"./data\", transform=transforms.ToTensor())\n",
        "testset=MNIST(root=\"./data\", train=False, transform=transforms.ToTensor())\n",
        "\n",
        "#plotting the dataset\n",
        "image,labels=dataset[1000]\n",
        "plt.imshow(image[0,10:25,10:25], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)"
      ],
      "metadata": {
        "id": "ZQV72q6kWpJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NHGGYphHtljA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the whole dataset into validation data and training data\n",
        "def split_data(dataset, validation_percent):\n",
        "  validation_data=int(dataset*validation_percent)\n",
        "  shuffled=np.random.permutation(dataset)\n",
        "  return shuffled[validation_data:], shuffled[:validation_data]\n",
        "\n",
        "training_data,validation_data = split_data(len(dataset), 0.3)\n",
        "print(\"length of training data: \", len(training_data))\n",
        "print(\"length of validation data: \", len(validation_data))\n",
        "\n",
        "print(\"portion of validation data: \",validation_data[:20])"
      ],
      "metadata": {
        "id": "dlorU70oXLps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting all the splitted data to the sampler and then into the dataloader\n",
        "train_data_sampler=SubsetRandomSampler(training_data)\n",
        "valid_data_sampler=SubsetRandomSampler(validation_data)\n",
        "\n",
        "batch_size=100\n",
        "\n",
        "training_loader=DataLoader(dataset, batch_size, sampler=train_data_sampler)\n",
        "validation_loader=DataLoader(dataset, batch_size, sampler=valid_data_sampler)"
      ],
      "metadata": {
        "id": "JIlbxkGddWAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the model\n",
        "input_size=28*28\n",
        "num_classes=10\n",
        "\n",
        "class MNISTmodel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.Linear=nn.Linear(input_size,num_classes)\n",
        "\n",
        "  def forward(self, size):\n",
        "    size=size.reshape(-1,784)\n",
        "    output=self.Linear(size)\n",
        "    return output\n",
        "\n",
        "model=MNISTmodel()"
      ],
      "metadata": {
        "id": "3Gej-4W87N2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting the training loader in the for loop as inputs and outputs for prediction\n",
        "for images, labels in training_loader:\n",
        "  prediction=model(images)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zZXbi9a5Pv87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display predictions and sum of the predictions of each array\n",
        "print(prediction[:2])\n",
        "sum=torch.sum(prediction[2])\n",
        "print(sum)\n",
        "\n",
        "#changing the sum of the probablities of the predictions close to 1 and then checking the sum again\n",
        "prob=F.softmax(prediction)\n",
        "print(prob[:2])\n",
        "sum=torch.sum(prob[2])\n",
        "print(sum)\n",
        "\n",
        "#displaying the exact predicted labels by the model\n",
        "max_prob, pred = torch.max(prob, dim=1)\n",
        "print(pred)\n",
        "print(max_prob)\n",
        "\n",
        "#displaying the actual target labels\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "XGqPcJUW9P4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the loss\n",
        "loss_fn=F.cross_entropy"
      ],
      "metadata": {
        "id": "ivMZt7kCgQQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the optimizer\n",
        "opt=torch.optim.SGD(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "iDN3RG9YjUEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy metrics\n",
        "def accuracy(outputs, labels):\n",
        "  _,pred=torch.max(outputs, dim=1)\n",
        "  return (torch.sum(pred==labels).item()/len(pred))*100"
      ],
      "metadata": {
        "id": "bSGzZGi4S59k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss batch function for loss computation, gradient computation, updating weights, resetting gradients, accuracy computation\n",
        "def loss_batch(model, loss_fn, images, labels, opt, metrics=accuracy):\n",
        "  prediction=model(images)\n",
        "  loss=loss_fn(prediction, labels)\n",
        "\n",
        "  if opt is not None:\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  metric_result=None\n",
        "  if metrics is not None:\n",
        "    metric_result=metrics(prediction, labels)\n",
        "\n",
        "  return loss.item(), len(images), metric_result"
      ],
      "metadata": {
        "id": "oObTH91hPsnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the average loss and average accuracy of the validation set\n",
        "def evaluate(model, loss_fn, validation_loader, metrics=accuracy):\n",
        "  with torch.no_grad():\n",
        "    validation_prediction=[loss_batch(model, loss_fn, images, labels, opt=None, metrics=accuracy) for images, labels in validation_loader]\n",
        "\n",
        "    losses, nums, metric=zip(*validation_prediction)\n",
        "\n",
        "    total=np.sum(nums)\n",
        "\n",
        "    average_loss = np.sum(np.multiply(losses, nums))/total\n",
        "\n",
        "    average_metrics=None\n",
        "    if metrics is not None:\n",
        "      average_metrics = np.sum(np.multiply(metric, nums))/total\n",
        "\n",
        "  return average_loss.item(), total, average_metrics"
      ],
      "metadata": {
        "id": "OkoWI6TfbQyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for explicit training\n",
        "def fit(nepochs, model, images, labels, training_loader, validation_loader, opt, metrics=accuracy):\n",
        "  for epoch in range(nepochs):\n",
        "    for images, labels in training_loader:\n",
        "      train_loss,_, train_accuracy=loss_batch(model, loss_fn, images, labels, opt, metrics=accuracy)\n",
        "\n",
        "    valid_loss, _, valid_accuracy= evaluate(model, loss_fn, validation_loader, metrics=accuracy)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{nepochs}\")\n",
        "    print(f\"Training loss: {train_loss:.4f} and Validation loss: {valid_loss:.4f}.\")\n",
        "    print(f\"Training accuracy: {train_accuracy:.2f}% and Validation accuracy: {valid_accuracy:.2f}%.\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return train_loss, _, train_accuracy, valid_loss, _, valid_accuracy\n",
        "\n",
        "train_loss,_, train_accuracy, valid_loss, _, valid_accuracy = fit(6, model, images, labels, training_loader, validation_loader, opt, metrics=accuracy)\n",
        "\n",
        "print(\"--\")\n",
        "print(f\"The train accuracy is {train_accuracy:.2f} % and loss is {train_loss:.4f}.\")\n",
        "print(\"--------------------------------------------\")\n",
        "print(f\"The validation accuracy is {valid_accuracy:.2f} % and loss is {valid_loss:.4f}\")"
      ],
      "metadata": {
        "id": "TCttIo85b4ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model with the testing dataset\n",
        "#function for predicting the test images\n",
        "def predict_image(image, model):\n",
        "  input=image.unsqueeze(0)\n",
        "  output=model(input)\n",
        "  _,preds=torch.max(output, dim=1)\n",
        "\n",
        "  return preds[0].item()"
      ],
      "metadata": {
        "id": "v8W1V8lqiu3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting and displaying different labels\n",
        "image,labels=testset[10]\n",
        "plt.imshow(image[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)\n",
        "print(\"predicted: \", predict_image(image, model))\n",
        "\n",
        "image,labels=testset[100]\n",
        "plt.imshow(image[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)\n",
        "print(\"predicted: \", predict_image(image, model))\n",
        "\n",
        "image,labels=testset[1000]\n",
        "plt.imshow(image[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)\n",
        "print(\"predicted: \", predict_image(image, model))\n",
        "\n",
        "image,labels=testset[905]\n",
        "plt.imshow(image[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)\n",
        "print(\"predicted: \", predict_image(image, model))"
      ],
      "metadata": {
        "id": "rmP4znRwkh7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the loss and accuracy on the test set\n",
        "test_loader=DataLoader(testset, batch_size=200)\n",
        "test_loss, total, test_accuracy=evaluate(model, loss_fn, test_loader, metrics=accuracy)\n",
        "print(f\"The test set loss is {test_loss:.4f} and the accuracy is {test_accuracy:.2f}%.\")"
      ],
      "metadata": {
        "id": "xCKU1FQKn-YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving and loading the model\n",
        "torch.save(model.state_dict(),'MNISTlogistic.pth')\n",
        "model.state_dict()"
      ],
      "metadata": {
        "id": "ILIq6GWqqLFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savedmodel=MNISTmodel()\n",
        "savedmodel.load_state_dict(torch.load('MNISTlogistic.pth'))\n",
        "savedmodel.state_dict()"
      ],
      "metadata": {
        "id": "9s_BDeUdr7ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNhqtZ1sYjVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPQfPo9TYjST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transform\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader"
      ],
      "metadata": {
        "id": "wu3GnbgisZBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "dataset=MNIST(root='./data', download=True, train=True, transform=transform.ToTensor())\n",
        "testset=MNIST(root='./data', download=True, train=False, transform=transform.ToTensor())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "shgd51IfVTNH",
        "outputId": "42db6c6e-d7b0-4daf-f5fc-0044221070de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 499kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.54MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.27MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "images, labels = dataset[230]\n",
        "plot=images[:,10:17,10:17]\n",
        "plt.imshow(plot[0], cmap='gray')\n",
        "plt.show()\n",
        "print(\"labels: \",labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "YXJRiX_MYaF5",
        "outputId": "c72ff623-ac4a-4580-b25e-2fdead5e416f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "def splitted_data(data, valid_data_percent):\n",
        "  valid_num=int(data*valid_data_percent)\n",
        "  index=np.random.permutation(data)\n",
        "  return index[valid_num:], index[:valid_num]\n",
        "training_data, validation_data=splitted_data(len(dataset), 0.25)\n",
        "print(\"Training data: \",len(training_data))\n",
        "print(\"Validation data\",len(validation_data))\n",
        "print(\"portion of validation data: \", validation_data[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjY91nW4YZ3Z",
        "outputId": "9dae6011-0f66-40b3-c189-9218ff7101a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data:  45000\n",
            "Validation data 15000\n",
            "portion of validation data:  [32759 56133 17262 47952  9365 10448 30704 39474  3544 40124  1550 29659\n",
            " 46993 39887  6374 38157 40372 51602 54359 43798]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "training_sampler=SubsetRandomSampler(training_data)\n",
        "validation_sampler=SubsetRandomSampler(validation_data)\n",
        "\n",
        "batch_size=100\n",
        "\n",
        "training_loader=DataLoader(dataset=dataset, batch_size=batch_size, sampler=training_sampler)\n",
        "validation_loader=DataLoader(dataset=dataset, batch_size=batch_size, sampler=validation_sampler)"
      ],
      "metadata": {
        "id": "1c2jumtnYZ0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "input_size=28*28\n",
        "hidden_size1=256\n",
        "hidden_size2=128\n",
        "output_size=10\n",
        "class MNISTMODEL(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size1, hidden_size2 ,output_size):\n",
        "    super().__init__()\n",
        "    #hidden layer1\n",
        "    self.linear1=nn.Linear(input_size, hidden_size1)\n",
        "    #hidden layer2\n",
        "    self.linear2=nn.Linear(hidden_size1, hidden_size2)\n",
        "    #output layer\n",
        "    self.linear3=nn.Linear(hidden_size2, output_size)\n",
        "\n",
        "\n",
        "  def forward(self,batch):\n",
        "    size=batch.view(batch.size(0),-1)\n",
        "\n",
        "    hidden1=self.linear1(size)\n",
        "\n",
        "    output=F.relu(hidden1)\n",
        "\n",
        "    hidden2=self.linear2(output)\n",
        "\n",
        "    output=F.relu(hidden2)\n",
        "\n",
        "    output=self.linear3(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "model=MNISTMODEL(input_size, hidden_size1, hidden_size2, output_size).to(device)"
      ],
      "metadata": {
        "id": "zlKTAD2HYZyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in model.parameters():\n",
        "  print(t.shape)\n",
        "  print(t.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M5CWRypoUj6",
        "outputId": "52619163-7b57-453a-d392-bb001cba515a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 784])\n",
            "cuda:0\n",
            "torch.Size([256])\n",
            "cuda:0\n",
            "torch.Size([128, 256])\n",
            "cuda:0\n",
            "torch.Size([128])\n",
            "cuda:0\n",
            "torch.Size([10, 128])\n",
            "cuda:0\n",
            "torch.Size([10])\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "for images, labels in training_loader:\n",
        "  images, labels = images.to(device), labels.to(device)\n",
        "  prediction=model(images)"
      ],
      "metadata": {
        "id": "4vmTOP1aYZue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "print(prediction[0])\n",
        "\n",
        "#9\n",
        "prob_sum=torch.sum(prediction[0])\n",
        "print(prob_sum)\n",
        "\n",
        "changed_pred=F.softmax(prediction, dim=1)\n",
        "changed_sum=torch.sum(changed_pred[9])\n",
        "print(changed_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26sKMZg6YZjv",
        "outputId": "10ecb15f-0cf0-45a2-b184-94a89f0043b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0171,  0.1247,  0.0518, -0.1265, -0.1295, -0.0360, -0.0045,  0.0653,\n",
            "         0.0805,  0.0610], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor(0.0696, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "_,pred=torch.max(prediction, dim=1)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8PPAvBlgqZA",
        "outputId": "a320cc74-510a-4437-ec56-7b4384a7a53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 8, 8, 1, 1, 1, 1, 8, 8, 1, 1, 8, 1, 8, 1, 1, 8, 8, 8, 1, 8, 1, 8, 1,\n",
            "        1, 1, 1, 8, 8, 1, 1, 8, 8, 1, 8, 1, 1, 1, 8, 1, 8, 8, 8, 1, 1, 8, 1, 8,\n",
            "        1, 1, 1, 1, 1, 1, 8, 8, 8, 8, 1, 1, 1, 8, 8, 1, 1, 1, 8, 1, 1, 1, 8, 1,\n",
            "        1, 8, 1, 8, 1, 1, 8, 8, 1, 8, 8, 1, 8, 1, 8, 8, 1, 1, 1, 1, 8, 1, 1, 1,\n",
            "        8, 1, 8, 8], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydw4qipQgqRs",
        "outputId": "e769a71b-5387-4eaf-e4fa-6334af9e6623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 4, 4, 5, 0, 3, 6, 7, 3, 8, 3, 2, 6, 2, 3, 3, 9, 2, 3, 5, 2, 1, 0, 0,\n",
              "        0, 9, 4, 9, 3, 8, 8, 8, 1, 8, 6, 2, 3, 8, 4, 4, 5, 7, 1, 6, 4, 7, 5, 1,\n",
              "        8, 9, 2, 8, 7, 5, 9, 1, 1, 8, 3, 0, 9, 3, 3, 8, 1, 5, 3, 0, 3, 2, 4, 4,\n",
              "        1, 5, 9, 1, 8, 5, 1, 3, 8, 0, 1, 0, 3, 1, 3, 8, 8, 5, 8, 7, 0, 7, 8, 2,\n",
              "        9, 1, 2, 2], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12\n",
        "def accuracy(output, labels):\n",
        "  _,pred=torch.max(output, dim=1)\n",
        "  return torch.sum(pred==labels).item()/len(pred)*100"
      ],
      "metadata": {
        "id": "AYn7pl5AYZaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13\n",
        "loss_function=F.cross_entropy"
      ],
      "metadata": {
        "id": "whNpBd-Vfcy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14\n",
        "opt=torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "5OhakYczfcwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15\n",
        "def loss_batch(model, loss_function, images, labels, opt, metrics=accuracy):\n",
        "  prediction=model(images.to(device))\n",
        "  loss=loss_function(prediction, labels.to(device))\n",
        "\n",
        "  if opt is not None:\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  metric_result=None\n",
        "  if metrics is not None:\n",
        "    metric_result=metrics(prediction, labels.to(device))\n",
        "\n",
        "  return loss.item(), len(images), metric_result"
      ],
      "metadata": {
        "id": "j0su4yJQfct6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16\n",
        "def evaluate(model, loss_function, validation_loader, metrics=accuracy):\n",
        "  with torch.no_grad():\n",
        "    result=[loss_batch(model, loss_function, images.to(device), labels.to(device), opt=None, metrics=accuracy) for images, labels in validation_loader]\n",
        "\n",
        "    losses, num, metric=zip(*result)\n",
        "\n",
        "    total=np.sum(num)\n",
        "\n",
        "    loss=np.sum(np.multiply(losses, num))/total\n",
        "\n",
        "    metric=None\n",
        "    if metrics is not None:\n",
        "      metric=np.sum(np.multiply(metric, num))/total\n",
        "  return loss, total, metric"
      ],
      "metadata": {
        "id": "3mu76XCEfcrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17\n",
        "def train(nepochs, model, loss_function, training_loader, validation_loader, images, labels, opt, metrics=accuracy):\n",
        "  for epoch in range(nepochs):\n",
        "    model.train()\n",
        "    for images, labels in training_loader:\n",
        "\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      train_loss,_,train_acc=loss_batch(model, loss_function, images.to(device), labels.to(device), opt, metrics=accuracy)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss,_,valid_acc=evaluate(model, loss_function, validation_loader, metrics=accuracy)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{nepochs}\")\n",
        "    print(f\"Training loss: {train_loss:.4f} and Validation loss: {valid_loss:.4f}.\")\n",
        "    print(f\"Training accuracy: {train_acc:.2f}% and Validation accuracy: {valid_acc:.2f}%.\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return train_loss,train_acc, valid_loss,valid_acc\n",
        "\n",
        "train_loss, train_acc, valid_loss, valid_acc = train(1, model, loss_function, training_loader, validation_loader, images, labels, opt, metrics=accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI5bHKeEfco7",
        "outputId": "e59841b8-4ec9-46eb-9e5b-33ed84d6318b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/1\n",
            "Training loss: 1.9298 and Validation loss: 1.9427.\n",
            "Training accuracy: 63.00% and Validation accuracy: 55.05%.\n",
            "--------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18\n",
        "def prediction(images, model):\n",
        "  input=images.to(device).unsqueeze(0)\n",
        "  output=model(input)\n",
        "  _,pred=torch.max(output, dim=1)\n",
        "\n",
        "  return pred[0].item()"
      ],
      "metadata": {
        "id": "cZ2_ceTcfcmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19\n",
        "images, labels=testset[6]\n",
        "plt.imshow(images[0], cmap='gray')\n",
        "plt.show()\n",
        "print(\"labels: \", labels)\n",
        "print(\"predicted: \",prediction(images.to(device), model))\n",
        "\n",
        "test_loader=DataLoader(testset, batch_size=200)\n",
        "\n",
        "\n",
        "#test accuracy (you define it in your own form)\n",
        "print(evaluate(model, loss_function, test_loader, metrics=accuracy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "CfzFPJNqfcjs",
        "outputId": "d9262a4f-f35b-40a0-91bb-7ccc9fcaddf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbe0lEQVR4nO3df2zU9R3H8dfx60Btr9TaXk9+WPAHi/wwY1IbteJoKJ0xosSoYxssRIcWM2HqUqPijyXdWNyMC+KWGCpTBFkGRLY1w2pLNguGYsd0W0O7utZAi5L1rhQpTfvZH8SbJy34Pe767rXPR/JJuO/3+77vm49f++J732+/53POOQEAMMhGWTcAABiZCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYGGPdwJf19fXp8OHDSktLk8/ns24HAOCRc06dnZ0KhUIaNWrg85whF0CHDx/W5MmTrdsAAJyn1tZWTZo0acD1Q+4juLS0NOsWAAAJcK6f50kLoPXr1+uyyy7T+PHjlZ+fr/fee+8r1fGxGwAMD+f6eZ6UANq6davWrFmjtWvX6sCBA5ozZ46Ki4t19OjRZOwOAJCKXBLMmzfPlZaWRl/39va6UCjkysvLz1kbDoedJAaDwWCk+AiHw2f9eZ/wM6BTp06prq5ORUVF0WWjRo1SUVGRamtrz9i+u7tbkUgkZgAAhr+EB9Cnn36q3t5e5eTkxCzPyclRW1vbGduXl5crEAhEB3fAAcDIYH4XXFlZmcLhcHS0trZatwQAGAQJ/z2grKwsjR49Wu3t7THL29vbFQwGz9je7/fL7/cnug0AwBCX8DOgcePGae7cuaqqqoou6+vrU1VVlQoKChK9OwBAikrKkxDWrFmjZcuW6Rvf+IbmzZun559/Xl1dXfr+97+fjN0BAFJQUgLorrvu0ieffKInn3xSbW1tuuaaa1RZWXnGjQkAgJHL55xz1k18USQSUSAQsG4DAHCewuGw0tPTB1xvfhccAGBkIoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGBijHUDGFmys7M917zxxhuea959913PNZL0m9/8xnPNRx99FNe+MHgCgUBcdYWFhZ5rKisrPdf09PR4rhkOOAMCAJgggAAAJhIeQE899ZR8Pl/MmDFjRqJ3AwBIcUm5BnT11Vfrrbfe+v9OxnCpCQAQKynJMGbMGAWDwWS8NQBgmEjKNaBDhw4pFApp2rRpWrp0qVpaWgbctru7W5FIJGYAAIa/hAdQfn6+KioqVFlZqQ0bNqi5uVk33nijOjs7+92+vLxcgUAgOiZPnpzolgAAQ1DCA6ikpER33nmnZs+ereLiYv3xj39UR0fHgL/LUVZWpnA4HB2tra2JbgkAMAQl/e6AjIwMXXnllWpsbOx3vd/vl9/vT3YbAIAhJum/B3T8+HE1NTUpNzc32bsCAKSQhAfQww8/rJqaGn300Ud69913dfvtt2v06NG65557Er0rAEAKS/hHcB9//LHuueceHTt2TJdccoluuOEG7d27V5dcckmidwUASGEJD6AtW7Yk+i0xRE2cONFzzYcffui5Jp4HSba3t3uukXiwaCqI53ioq6uLa1/x/MN57ty5nmsGukY+3PEsOACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACaS/oV0GPqysrLiqtu6davnmszMTM81L774oueaBx980HMNUsPjjz/uuSYvLy+uff3gBz/wXDNSHywaD86AAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmfM45Z93EF0UiEQUCAes2RpSFCxfGVfenP/0pwZ30LxgMeq755JNPktAJEu3qq6/2XPP3v//dc8327ds910jS8uXLPdd0dnbGta/hKBwOKz09fcD1nAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwMca6ASRWdna255olS5YkoZP+rVixwnMNDxZNDfE8WPStt95KQidnivdhpDxYNLk4AwIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCh5EOM88995znmu985ztx7auurs5zzbZt2+LaF4a+G2+80XNNTk6O55qKigrPNa+++qrnGiQfZ0AAABMEEADAhOcA2rNnj2699VaFQiH5fD7t2LEjZr1zTk8++aRyc3M1YcIEFRUV6dChQ4nqFwAwTHgOoK6uLs2ZM0fr16/vd/26dev0wgsv6KWXXtK+fft04YUXqri4WCdPnjzvZgEAw4fnmxBKSkpUUlLS7zrnnJ5//nk9/vjjuu222yRJmzZtUk5Ojnbs2KG77777/LoFAAwbCb0G1NzcrLa2NhUVFUWXBQIB5efnq7a2tt+a7u5uRSKRmAEAGP4SGkBtbW2Szry1MicnJ7ruy8rLyxUIBKJj8uTJiWwJADBEmd8FV1ZWpnA4HB2tra3WLQEABkFCAygYDEqS2tvbY5a3t7dH132Z3+9Xenp6zAAADH8JDaC8vDwFg0FVVVVFl0UiEe3bt08FBQWJ3BUAIMV5vgvu+PHjamxsjL5ubm5WfX29MjMzNWXKFD300EP6yU9+oiuuuEJ5eXl64oknFAqFtHjx4kT2DQBIcZ4DaP/+/br55pujr9esWSNJWrZsmSoqKvToo4+qq6tL9913nzo6OnTDDTeosrJS48ePT1zXAICU53POOesmvigSiSgQCFi3kbI2bdrkuWbp0qVx7esPf/iD55olS5Z4runp6fFcg9MmTJgQV91jjz3mueaBBx7wXJORkeG5ZvTo0Z5rYCMcDp/1ur75XXAAgJGJAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGDC89cxAJ+75ZZbPNf8+c9/9lzT0dHhuWbDhg2ea4a6m266yXPN/Pnz49rXddddF1edV7/73e8GZT8YmjgDAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYMLnnHPWTXxRJBJRIBCwbiNlzZ0713PNjh074tpXKBSKq84rn8/nuWaIHdYJMdTn4d///rfnmkWLFnmuaWpq8lwDG+FwWOnp6QOu5wwIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAiTHWDSCx6urqPNfMnj07rn1dc801nmviefjkI4884rnmk08+8VwjSa+88kpcdYPht7/9reeav/3tb0nopH/vvvuu5xoeLDqycQYEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADAhM8556yb+KJIJKJAIGDdBjDkTJs2zXNNY2NjXPuqr6/3XFNcXOy5Jt6HxiI1hMNhpaenD7ieMyAAgAkCCABgwnMA7dmzR7feeqtCoZB8Pp927NgRs3758uXy+XwxI57vgAEADG+eA6irq0tz5szR+vXrB9xm0aJFOnLkSHS8/vrr59UkAGD48fyNqCUlJSopKTnrNn6/X8FgMO6mAADDX1KuAVVXVys7O1tXXXWV7r//fh07dmzAbbu7uxWJRGIGAGD4S3gALVq0SJs2bVJVVZV+9rOfqaamRiUlJert7e13+/LycgUCgeiYPHlyolsCAAxBnj+CO5e77747+udZs2Zp9uzZmj59uqqrq7VgwYIzti8rK9OaNWuiryORCCEEACNA0m/DnjZtmrKysgb8hTi/36/09PSYAQAY/pIeQB9//LGOHTum3NzcZO8KAJBCPH8Ed/z48ZizmebmZtXX1yszM1OZmZl6+umntWTJEgWDQTU1NenRRx/V5ZdfHtdjOgAAw5fnANq/f79uvvnm6OvPr98sW7ZMGzZs0MGDB/XKK6+oo6NDoVBICxcu1LPPPiu/35+4rgEAKY+HkQIpoqKiwnPNd7/73bj2Fc/TS3bv3h3XvjB88TBSAMCQRAABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwkfCv5AZwbnfeeafnmu9973ueazo7Oz3XSNKxY8fiqgO84AwIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACR5GChgoKSkZlP3s2rUrrroDBw4kuBPgTJwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMHDSAED8TyMtKury3PNc88957kGGCycAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDBw0iB87Ry5UrPNTk5OZ5rjh496rnmwIEDnmuAwcIZEADABAEEADDhKYDKy8t17bXXKi0tTdnZ2Vq8eLEaGhpitjl58qRKS0t18cUX66KLLtKSJUvU3t6e0KYBAKnPUwDV1NSotLRUe/fu1e7du9XT06OFCxfGfFHW6tWr9eabb2rbtm2qqanR4cOHdccddyS8cQBAavN0E0JlZWXM64qKCmVnZ6uurk6FhYUKh8N6+eWXtXnzZn3zm9+UJG3cuFFf+9rXtHfvXl133XWJ6xwAkNLO6xpQOByWJGVmZkqS6urq1NPTo6Kioug2M2bM0JQpU1RbW9vve3R3dysSicQMAMDwF3cA9fX16aGHHtL111+vmTNnSpLa2to0btw4ZWRkxGybk5Ojtra2ft+nvLxcgUAgOiZPnhxvSwCAFBJ3AJWWluqDDz7Qli1bzquBsrIyhcPh6GhtbT2v9wMApIa4fhF11apV2rVrl/bs2aNJkyZFlweDQZ06dUodHR0xZ0Ht7e0KBoP9vpff75ff74+nDQBACvN0BuSc06pVq7R9+3a9/fbbysvLi1k/d+5cjR07VlVVVdFlDQ0NamlpUUFBQWI6BgAMC57OgEpLS7V582bt3LlTaWlp0es6gUBAEyZMUCAQ0IoVK7RmzRplZmYqPT1dDz74oAoKCrgDDgAQw1MAbdiwQZI0f/78mOUbN27U8uXLJUm//OUvNWrUKC1ZskTd3d0qLi7Wiy++mJBmAQDDh88556yb+KJIJKJAIGDdBvCV1dfXe66ZNWuW55qKigrPNStWrPBcI0lpaWmeayZOnOi5pqWlxXMNUkc4HFZ6evqA63kWHADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARFzfiApg8PX29nquWbp0aVz7Wr16teeaDz/80HPNsmXLPNdg+OAMCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAmfc85ZN/FFkUhEgUDAug3gK6uvr/dcM2vWLM81Pp/Pc028/3u//PLLnmueffZZzzWtra2ea5A6wuGw0tPTB1zPGRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATY6wbAFLdqlWrPNc888wznmv27NnjuWbDhg2eayTpv//9r+eaU6dOxbUvjFycAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDhc8456ya+KBKJKBAIWLcBADhP4XBY6enpA67nDAgAYIIAAgCY8BRA5eXluvbaa5WWlqbs7GwtXrxYDQ0NMdvMnz9fPp8vZqxcuTKhTQMAUp+nAKqpqVFpaan27t2r3bt3q6enRwsXLlRXV1fMdvfee6+OHDkSHevWrUto0wCA1OfpG1ErKytjXldUVCg7O1t1dXUqLCyMLr/gggsUDAYT0yEAYFg6r2tA4XBYkpSZmRmz/LXXXlNWVpZmzpypsrIynThxYsD36O7uViQSiRkAgBHAxam3t9fdcsst7vrrr49Z/utf/9pVVla6gwcPuldffdVdeuml7vbbbx/wfdauXeskMRgMBmOYjXA4fNYciTuAVq5c6aZOnepaW1vPul1VVZWT5BobG/tdf/LkSRcOh6OjtbXVfNIYDAaDcf7jXAHk6RrQ51atWqVdu3Zpz549mjRp0lm3zc/PlyQ1NjZq+vTpZ6z3+/3y+/3xtAEASGGeAsg5pwcffFDbt29XdXW18vLyzllTX18vScrNzY2rQQDA8OQpgEpLS7V582bt3LlTaWlpamtrkyQFAgFNmDBBTU1N2rx5s771rW/p4osv1sGDB7V69WoVFhZq9uzZSfkLAABSlJfrPhrgc76NGzc655xraWlxhYWFLjMz0/n9fnf55Ze7Rx555JyfA35ROBw2/9ySwWAwGOc/zvWzn4eRAgCSgoeRAgCGJAIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAiSEXQM456xYAAAlwrp/nQy6AOjs7rVsAACTAuX6e+9wQO+Xo6+vT4cOHlZaWJp/PF7MuEolo8uTJam1tVXp6ulGH9piH05iH05iH05iH04bCPDjn1NnZqVAopFGjBj7PGTOIPX0lo0aN0qRJk866TXp6+og+wD7HPJzGPJzGPJzGPJxmPQ+BQOCc2wy5j+AAACMDAQQAMJFSAeT3+7V27Vr5/X7rVkwxD6cxD6cxD6cxD6el0jwMuZsQAAAjQ0qdAQEAhg8CCABgggACAJgggAAAJlImgNavX6/LLrtM48ePV35+vt577z3rlgbdU089JZ/PFzNmzJhh3VbS7dmzR7feeqtCoZB8Pp927NgRs945pyeffFK5ubmaMGGCioqKdOjQIZtmk+hc87B8+fIzjo9FixbZNJsk5eXluvbaa5WWlqbs7GwtXrxYDQ0NMducPHlSpaWluvjii3XRRRdpyZIlam9vN+o4Ob7KPMyfP/+M42HlypVGHfcvJQJo69atWrNmjdauXasDBw5ozpw5Ki4u1tGjR61bG3RXX321jhw5Eh1/+ctfrFtKuq6uLs2ZM0fr16/vd/26dev0wgsv6KWXXtK+fft04YUXqri4WCdPnhzkTpPrXPMgSYsWLYo5Pl5//fVB7DD5ampqVFpaqr1792r37t3q6enRwoUL1dXVFd1m9erVevPNN7Vt2zbV1NTo8OHDuuOOOwy7TryvMg+SdO+998YcD+vWrTPqeAAuBcybN8+VlpZGX/f29rpQKOTKy8sNuxp8a9eudXPmzLFuw5Qkt3379ujrvr4+FwwG3c9//vPoso6ODuf3+93rr79u0OHg+PI8OOfcsmXL3G233WbSj5WjR486Sa6mpsY5d/q//dixY922bdui2/zzn/90klxtba1Vm0n35XlwzrmbbrrJ/fCHP7Rr6isY8mdAp06dUl1dnYqKiqLLRo0apaKiItXW1hp2ZuPQoUMKhUKaNm2ali5dqpaWFuuWTDU3N6utrS3m+AgEAsrPzx+Rx0d1dbWys7N11VVX6f7779exY8esW0qqcDgsScrMzJQk1dXVqaenJ+Z4mDFjhqZMmTKsj4cvz8PnXnvtNWVlZWnmzJkqKyvTiRMnLNob0JB7GOmXffrpp+rt7VVOTk7M8pycHP3rX/8y6spGfn6+KioqdNVVV+nIkSN6+umndeONN+qDDz5QWlqadXsm2traJKnf4+PzdSPFokWLdMcddygvL09NTU167LHHVFJSotraWo0ePdq6vYTr6+vTQw89pOuvv14zZ86UdPp4GDdunDIyMmK2Hc7HQ3/zIEnf/va3NXXqVIVCIR08eFA//vGP1dDQoN///veG3cYa8gGE/yspKYn+efbs2crPz9fUqVP1xhtvaMWKFYadYSi4++67o3+eNWuWZs+erenTp6u6uloLFiww7Cw5SktL9cEHH4yI66BnM9A83HfffdE/z5o1S7m5uVqwYIGampo0ffr0wW6zX0P+I7isrCyNHj36jLtY2tvbFQwGjboaGjIyMnTllVeqsbHRuhUznx8DHB9nmjZtmrKysobl8bFq1Srt2rVL77zzTszXtwSDQZ06dUodHR0x2w/X42GgeehPfn6+JA2p42HIB9C4ceM0d+5cVVVVRZf19fWpqqpKBQUFhp3ZO378uJqampSbm2vdipm8vDwFg8GY4yMSiWjfvn0j/vj4+OOPdezYsWF1fDjntGrVKm3fvl1vv/228vLyYtbPnTtXY8eOjTkeGhoa1NLSMqyOh3PNQ3/q6+slaWgdD9Z3QXwVW7ZscX6/31VUVLh//OMf7r777nMZGRmura3NurVB9aMf/chVV1e75uZm99e//tUVFRW5rKwsd/ToUevWkqqzs9O9//777v3333eS3C9+8Qv3/vvvu//85z/OOed++tOfuoyMDLdz50538OBBd9ttt7m8vDz32WefGXeeWGebh87OTvfwww+72tpa19zc7N566y339a9/3V1xxRXu5MmT1q0nzP333+8CgYCrrq52R44ciY4TJ05Et1m5cqWbMmWKe/vtt93+/ftdQUGBKygoMOw68c41D42Nje6ZZ55x+/fvd83NzW7nzp1u2rRprrCw0LjzWCkRQM4596tf/cpNmTLFjRs3zs2bN8/t3bvXuqVBd9ddd7nc3Fw3btw4d+mll7q77rrLNTY2WreVdO+8846TdMZYtmyZc+70rdhPPPGEy8nJcX6/3y1YsMA1NDTYNp0EZ5uHEydOuIULF7pLLrnEjR071k2dOtXde++9w+4faf39/SW5jRs3Rrf57LPP3AMPPOAmTpzoLrjgAnf77be7I0eO2DWdBOeah5aWFldYWOgyMzOd3+93l19+uXvkkUdcOBy2bfxL+DoGAICJIX8NCAAwPBFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDxP0qNyc3fKb8QAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  4\n",
            "predicted:  9\n",
            "(1.9349282193183899, 10000, 55.52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20\n",
        "torch.save(model.state_dict(), \"MNIST.pth\")\n",
        "model.state_dict()"
      ],
      "metadata": {
        "id": "p9Tq1Ttsfbyz",
        "outputId": "f044bc5d-72c0-4e7a-a102-f341c7797895",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight',\n",
              "              tensor([[-1.6056e-02,  2.6531e-02, -2.3816e-02,  ...,  3.6022e-03,\n",
              "                        8.7305e-03, -4.2433e-03],\n",
              "                      [ 4.9940e-06, -1.2903e-02,  1.5778e-02,  ...,  3.3819e-02,\n",
              "                        8.5465e-03, -2.8956e-02],\n",
              "                      [ 3.3954e-02,  1.6042e-02, -4.4530e-03,  ...,  9.0182e-03,\n",
              "                        2.7589e-02,  2.2246e-02],\n",
              "                      ...,\n",
              "                      [ 2.3653e-02,  4.7523e-04,  4.2325e-03,  ..., -3.4262e-02,\n",
              "                       -3.0713e-02,  5.4511e-03],\n",
              "                      [-1.9246e-03,  3.5633e-02, -9.3522e-03,  ...,  1.0997e-02,\n",
              "                       -3.2198e-02,  2.2668e-02],\n",
              "                      [ 2.3678e-02,  1.2333e-02, -5.0494e-03,  ..., -3.0182e-02,\n",
              "                        8.2871e-03,  2.1086e-02]], device='cuda:0')),\n",
              "             ('linear1.bias',\n",
              "              tensor([ 1.2362e-02, -1.8548e-02,  3.1944e-02, -2.4508e-02,  3.0737e-03,\n",
              "                      -3.1868e-02, -1.9955e-03,  2.3316e-03, -1.7418e-02,  3.8623e-02,\n",
              "                       3.5868e-02, -2.3433e-03, -2.2256e-02,  2.8538e-02,  2.6633e-02,\n",
              "                      -3.1732e-02,  1.7410e-02,  3.0483e-03, -2.7250e-02, -2.1084e-02,\n",
              "                       1.5570e-02,  3.8222e-02,  3.7092e-03,  1.8857e-02,  5.5826e-03,\n",
              "                       3.5007e-02,  2.6975e-02,  1.3501e-02,  1.7390e-02,  9.7631e-03,\n",
              "                       2.8376e-02, -1.9062e-02,  2.2722e-02,  7.1927e-03,  1.8836e-02,\n",
              "                      -1.5880e-02,  1.2619e-02, -2.6132e-03, -3.3939e-02, -3.1906e-02,\n",
              "                       3.0703e-02,  1.8258e-02,  4.0592e-02, -4.7852e-03,  2.2652e-02,\n",
              "                       3.4279e-02,  2.3486e-02,  6.0429e-03,  2.3582e-02,  2.7367e-02,\n",
              "                      -9.5236e-03,  7.6308e-04,  2.8207e-02, -2.6037e-03, -8.8048e-03,\n",
              "                      -2.6254e-03, -2.5494e-02, -2.0743e-02, -5.8619e-03, -1.2424e-02,\n",
              "                      -2.8604e-02,  2.3120e-02,  1.3258e-02,  3.5496e-02,  2.6547e-02,\n",
              "                       2.2801e-02,  1.8325e-02,  3.6119e-02,  1.9081e-02, -2.0712e-03,\n",
              "                       2.4160e-02,  2.9907e-02, -2.6181e-02, -2.6178e-02,  3.2035e-02,\n",
              "                      -1.6580e-02,  3.0824e-02,  1.8719e-02, -1.5536e-02, -1.6352e-02,\n",
              "                       1.7342e-02,  1.3134e-02, -1.1786e-02,  5.8423e-03, -6.2526e-03,\n",
              "                      -4.7258e-05, -2.0752e-02,  5.4190e-04, -1.2510e-02,  1.5653e-03,\n",
              "                      -1.6956e-02, -5.5249e-03, -2.6660e-02, -1.1563e-02,  8.1536e-03,\n",
              "                      -2.4273e-02, -2.8561e-02,  2.3610e-02, -8.3899e-03, -3.8731e-03,\n",
              "                      -2.6293e-02, -1.1906e-02,  1.9927e-02, -3.1355e-02, -3.1762e-02,\n",
              "                       1.9569e-02, -4.9338e-03, -2.0585e-02,  7.0273e-03,  2.7096e-02,\n",
              "                      -3.1185e-02,  1.0638e-02, -1.3567e-02, -7.3089e-03, -2.7047e-02,\n",
              "                      -3.5785e-02, -2.4496e-02,  2.4606e-02,  5.5290e-03,  2.3413e-02,\n",
              "                      -9.5483e-03,  2.9126e-02,  6.6296e-03, -2.6945e-02,  9.5731e-03,\n",
              "                       3.0048e-02, -1.3047e-02, -1.1954e-03, -3.4875e-02, -2.9356e-02,\n",
              "                       4.7269e-03,  1.8731e-02,  2.6561e-03,  1.3085e-02, -1.1853e-02,\n",
              "                       1.8748e-02, -3.4374e-02, -7.7178e-03, -9.9656e-03,  1.5530e-02,\n",
              "                      -1.2438e-02,  4.5446e-02,  2.8615e-02,  3.6245e-02, -2.4127e-02,\n",
              "                       1.3355e-02,  2.1633e-02,  3.7304e-02, -7.9022e-03, -2.4383e-02,\n",
              "                       3.8059e-02, -3.7572e-03,  2.0115e-02, -3.0166e-02, -3.2019e-02,\n",
              "                      -1.6261e-02, -1.4494e-02,  1.6452e-03, -1.6245e-03,  1.0639e-02,\n",
              "                      -1.4880e-02,  2.7835e-02,  1.2556e-02,  1.4470e-03,  3.0976e-02,\n",
              "                       1.9950e-02,  1.0300e-05, -2.0474e-02, -2.3438e-02,  3.0784e-02,\n",
              "                      -4.4543e-03,  3.8044e-03,  3.0745e-02,  1.4746e-02,  2.0648e-02,\n",
              "                       3.0114e-02,  1.4382e-02,  1.3725e-02, -1.8372e-02,  3.4889e-02,\n",
              "                       6.3912e-03, -8.0367e-03,  2.3965e-02,  3.2454e-03,  2.2675e-02,\n",
              "                      -5.0216e-03,  6.1412e-04, -2.0381e-02, -6.0779e-03,  1.3470e-02,\n",
              "                      -1.6518e-02, -9.9100e-03,  2.3957e-02, -9.3047e-03, -3.4207e-02,\n",
              "                      -3.1237e-02,  2.7500e-02,  3.0203e-02, -2.7857e-02, -1.1931e-02,\n",
              "                      -2.3665e-03, -1.1826e-02,  1.9037e-02, -2.5638e-02, -3.0750e-02,\n",
              "                       1.4498e-02,  3.8960e-03, -2.3831e-02,  2.9364e-02,  2.6382e-02,\n",
              "                       2.0403e-02, -2.1801e-02,  7.3462e-04,  3.1129e-02, -1.8661e-02,\n",
              "                      -1.6736e-02,  2.9951e-02,  4.7695e-03, -1.3515e-02,  2.8582e-02,\n",
              "                       3.0099e-02,  2.4133e-02, -1.1320e-02,  4.0841e-02, -1.8126e-02,\n",
              "                       3.3044e-02,  5.7223e-03,  3.1383e-02,  1.4446e-02,  1.9359e-02,\n",
              "                       1.1889e-02, -2.0711e-02, -1.0540e-02, -1.0540e-02, -1.5589e-02,\n",
              "                       2.1595e-02, -2.8213e-02,  2.3510e-02,  2.2833e-02,  2.8684e-02,\n",
              "                       3.1144e-02, -6.1653e-03, -7.8088e-03, -1.7869e-02,  2.6396e-02,\n",
              "                      -2.5210e-02, -2.2276e-02, -2.3298e-02,  2.9436e-03,  1.1561e-02,\n",
              "                      -1.8241e-02, -2.4822e-02, -1.1524e-02,  2.4723e-02, -1.0332e-02,\n",
              "                       3.1241e-03], device='cuda:0')),\n",
              "             ('linear2.weight',\n",
              "              tensor([[-1.7172e-02, -5.0375e-02,  3.9796e-02,  ..., -5.8451e-03,\n",
              "                       -2.0116e-03,  2.4505e-03],\n",
              "                      [ 4.9952e-02, -3.2777e-02, -4.3084e-06,  ..., -1.7434e-02,\n",
              "                        4.2264e-02, -5.4219e-02],\n",
              "                      [-4.4808e-02,  2.4793e-02,  2.5012e-02,  ..., -4.0405e-02,\n",
              "                       -3.7764e-02,  4.2145e-02],\n",
              "                      ...,\n",
              "                      [-4.2247e-02, -2.0703e-02,  1.4359e-02,  ...,  7.3923e-03,\n",
              "                       -2.2167e-02, -4.5863e-02],\n",
              "                      [-4.4202e-02, -3.0057e-03, -4.9013e-02,  ...,  1.0297e-02,\n",
              "                        5.9328e-02,  3.3690e-03],\n",
              "                      [-9.2458e-03,  2.1795e-03,  5.4749e-02,  ..., -1.8542e-02,\n",
              "                        4.8148e-02, -2.3050e-02]], device='cuda:0')),\n",
              "             ('linear2.bias',\n",
              "              tensor([-0.0010, -0.0363,  0.0553,  0.0008,  0.0282, -0.0443,  0.0161,  0.0751,\n",
              "                       0.0065,  0.0779,  0.0544, -0.0347, -0.0092,  0.0784,  0.0434, -0.0464,\n",
              "                       0.0586, -0.0281,  0.0328,  0.0042,  0.0548, -0.0526,  0.0043, -0.0204,\n",
              "                      -0.0011,  0.0150,  0.0569, -0.0602, -0.0309,  0.0596,  0.0171,  0.0755,\n",
              "                       0.0155,  0.0474, -0.0397,  0.0455,  0.0216,  0.0092, -0.0298,  0.0334,\n",
              "                       0.0613,  0.0618,  0.0553,  0.0016,  0.0538,  0.0175,  0.0538,  0.0115,\n",
              "                      -0.0006, -0.0258,  0.0082, -0.0183,  0.0241, -0.0069, -0.0248, -0.0236,\n",
              "                       0.0357, -0.0322, -0.0568,  0.0145,  0.0406, -0.0327,  0.0216,  0.0279,\n",
              "                      -0.0264,  0.0480, -0.0003, -0.0173,  0.0228,  0.0103,  0.0041,  0.0416,\n",
              "                       0.0619, -0.0296, -0.0028,  0.0037,  0.0115,  0.0228, -0.0223, -0.0160,\n",
              "                      -0.0129, -0.0525,  0.0473,  0.0663, -0.0289, -0.0147, -0.0234, -0.0592,\n",
              "                      -0.0078,  0.0240,  0.0081,  0.0635,  0.0171,  0.0484,  0.0159,  0.0680,\n",
              "                       0.0299,  0.0428, -0.0054, -0.0007,  0.0660,  0.0332, -0.0108, -0.0501,\n",
              "                       0.0454, -0.0075,  0.0575, -0.0469, -0.0178,  0.0361,  0.0130, -0.0320,\n",
              "                      -0.0023, -0.0133, -0.0140, -0.0512,  0.0417,  0.0059, -0.0227,  0.0326,\n",
              "                      -0.0367, -0.0502,  0.0368,  0.0389,  0.0185, -0.0625,  0.0668, -0.0353],\n",
              "                     device='cuda:0')),\n",
              "             ('linear3.weight',\n",
              "              tensor([[ 0.0112,  0.0004,  0.1351,  ..., -0.0584, -0.0710, -0.0049],\n",
              "                      [ 0.0382, -0.0142,  0.0658,  ...,  0.0208,  0.1280,  0.0087],\n",
              "                      [ 0.0659,  0.0024,  0.0011,  ...,  0.0644, -0.0120,  0.0409],\n",
              "                      ...,\n",
              "                      [ 0.0601, -0.0449,  0.0078,  ..., -0.0361, -0.1011,  0.0469],\n",
              "                      [ 0.0455, -0.0312,  0.0386,  ...,  0.0216,  0.0340,  0.0550],\n",
              "                      [ 0.0331, -0.0174, -0.0074,  ..., -0.0450,  0.0159,  0.0165]],\n",
              "                     device='cuda:0')),\n",
              "             ('linear3.bias',\n",
              "              tensor([-0.0508,  0.0692,  0.0670, -0.0507, -0.0283, -0.0093, -0.0152,  0.0756,\n",
              "                       0.0099, -0.0155], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model=MNISTMODEL(input_size, hidden_size1, hidden_size2, output_size)\n",
        "saved_model.load_state_dict(torch.load('MNIST.pth'))\n",
        "saved_model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgEx63M7ouwh",
        "outputId": "fb713195-288d-4d46-d61f-35db99de09f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-450330895b00>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved_model.load_state_dict(torch.load('MNIST.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight',\n",
              "              tensor([[-1.6056e-02,  2.6531e-02, -2.3816e-02,  ...,  3.6022e-03,\n",
              "                        8.7305e-03, -4.2433e-03],\n",
              "                      [ 4.9940e-06, -1.2903e-02,  1.5778e-02,  ...,  3.3819e-02,\n",
              "                        8.5465e-03, -2.8956e-02],\n",
              "                      [ 3.3954e-02,  1.6042e-02, -4.4530e-03,  ...,  9.0182e-03,\n",
              "                        2.7589e-02,  2.2246e-02],\n",
              "                      ...,\n",
              "                      [ 2.3653e-02,  4.7523e-04,  4.2325e-03,  ..., -3.4262e-02,\n",
              "                       -3.0713e-02,  5.4511e-03],\n",
              "                      [-1.9246e-03,  3.5633e-02, -9.3522e-03,  ...,  1.0997e-02,\n",
              "                       -3.2198e-02,  2.2668e-02],\n",
              "                      [ 2.3678e-02,  1.2333e-02, -5.0494e-03,  ..., -3.0182e-02,\n",
              "                        8.2871e-03,  2.1086e-02]])),\n",
              "             ('linear1.bias',\n",
              "              tensor([ 1.2362e-02, -1.8548e-02,  3.1944e-02, -2.4508e-02,  3.0737e-03,\n",
              "                      -3.1868e-02, -1.9955e-03,  2.3316e-03, -1.7418e-02,  3.8623e-02,\n",
              "                       3.5868e-02, -2.3433e-03, -2.2256e-02,  2.8538e-02,  2.6633e-02,\n",
              "                      -3.1732e-02,  1.7410e-02,  3.0483e-03, -2.7250e-02, -2.1084e-02,\n",
              "                       1.5570e-02,  3.8222e-02,  3.7092e-03,  1.8857e-02,  5.5826e-03,\n",
              "                       3.5007e-02,  2.6975e-02,  1.3501e-02,  1.7390e-02,  9.7631e-03,\n",
              "                       2.8376e-02, -1.9062e-02,  2.2722e-02,  7.1927e-03,  1.8836e-02,\n",
              "                      -1.5880e-02,  1.2619e-02, -2.6132e-03, -3.3939e-02, -3.1906e-02,\n",
              "                       3.0703e-02,  1.8258e-02,  4.0592e-02, -4.7852e-03,  2.2652e-02,\n",
              "                       3.4279e-02,  2.3486e-02,  6.0429e-03,  2.3582e-02,  2.7367e-02,\n",
              "                      -9.5236e-03,  7.6308e-04,  2.8207e-02, -2.6037e-03, -8.8048e-03,\n",
              "                      -2.6254e-03, -2.5494e-02, -2.0743e-02, -5.8619e-03, -1.2424e-02,\n",
              "                      -2.8604e-02,  2.3120e-02,  1.3258e-02,  3.5496e-02,  2.6547e-02,\n",
              "                       2.2801e-02,  1.8325e-02,  3.6119e-02,  1.9081e-02, -2.0712e-03,\n",
              "                       2.4160e-02,  2.9907e-02, -2.6181e-02, -2.6178e-02,  3.2035e-02,\n",
              "                      -1.6580e-02,  3.0824e-02,  1.8719e-02, -1.5536e-02, -1.6352e-02,\n",
              "                       1.7342e-02,  1.3134e-02, -1.1786e-02,  5.8423e-03, -6.2526e-03,\n",
              "                      -4.7258e-05, -2.0752e-02,  5.4190e-04, -1.2510e-02,  1.5653e-03,\n",
              "                      -1.6956e-02, -5.5249e-03, -2.6660e-02, -1.1563e-02,  8.1536e-03,\n",
              "                      -2.4273e-02, -2.8561e-02,  2.3610e-02, -8.3899e-03, -3.8731e-03,\n",
              "                      -2.6293e-02, -1.1906e-02,  1.9927e-02, -3.1355e-02, -3.1762e-02,\n",
              "                       1.9569e-02, -4.9338e-03, -2.0585e-02,  7.0273e-03,  2.7096e-02,\n",
              "                      -3.1185e-02,  1.0638e-02, -1.3567e-02, -7.3089e-03, -2.7047e-02,\n",
              "                      -3.5785e-02, -2.4496e-02,  2.4606e-02,  5.5290e-03,  2.3413e-02,\n",
              "                      -9.5483e-03,  2.9126e-02,  6.6296e-03, -2.6945e-02,  9.5731e-03,\n",
              "                       3.0048e-02, -1.3047e-02, -1.1954e-03, -3.4875e-02, -2.9356e-02,\n",
              "                       4.7269e-03,  1.8731e-02,  2.6561e-03,  1.3085e-02, -1.1853e-02,\n",
              "                       1.8748e-02, -3.4374e-02, -7.7178e-03, -9.9656e-03,  1.5530e-02,\n",
              "                      -1.2438e-02,  4.5446e-02,  2.8615e-02,  3.6245e-02, -2.4127e-02,\n",
              "                       1.3355e-02,  2.1633e-02,  3.7304e-02, -7.9022e-03, -2.4383e-02,\n",
              "                       3.8059e-02, -3.7572e-03,  2.0115e-02, -3.0166e-02, -3.2019e-02,\n",
              "                      -1.6261e-02, -1.4494e-02,  1.6452e-03, -1.6245e-03,  1.0639e-02,\n",
              "                      -1.4880e-02,  2.7835e-02,  1.2556e-02,  1.4470e-03,  3.0976e-02,\n",
              "                       1.9950e-02,  1.0300e-05, -2.0474e-02, -2.3438e-02,  3.0784e-02,\n",
              "                      -4.4543e-03,  3.8044e-03,  3.0745e-02,  1.4746e-02,  2.0648e-02,\n",
              "                       3.0114e-02,  1.4382e-02,  1.3725e-02, -1.8372e-02,  3.4889e-02,\n",
              "                       6.3912e-03, -8.0367e-03,  2.3965e-02,  3.2454e-03,  2.2675e-02,\n",
              "                      -5.0216e-03,  6.1412e-04, -2.0381e-02, -6.0779e-03,  1.3470e-02,\n",
              "                      -1.6518e-02, -9.9100e-03,  2.3957e-02, -9.3047e-03, -3.4207e-02,\n",
              "                      -3.1237e-02,  2.7500e-02,  3.0203e-02, -2.7857e-02, -1.1931e-02,\n",
              "                      -2.3665e-03, -1.1826e-02,  1.9037e-02, -2.5638e-02, -3.0750e-02,\n",
              "                       1.4498e-02,  3.8960e-03, -2.3831e-02,  2.9364e-02,  2.6382e-02,\n",
              "                       2.0403e-02, -2.1801e-02,  7.3462e-04,  3.1129e-02, -1.8661e-02,\n",
              "                      -1.6736e-02,  2.9951e-02,  4.7695e-03, -1.3515e-02,  2.8582e-02,\n",
              "                       3.0099e-02,  2.4133e-02, -1.1320e-02,  4.0841e-02, -1.8126e-02,\n",
              "                       3.3044e-02,  5.7223e-03,  3.1383e-02,  1.4446e-02,  1.9359e-02,\n",
              "                       1.1889e-02, -2.0711e-02, -1.0540e-02, -1.0540e-02, -1.5589e-02,\n",
              "                       2.1595e-02, -2.8213e-02,  2.3510e-02,  2.2833e-02,  2.8684e-02,\n",
              "                       3.1144e-02, -6.1653e-03, -7.8088e-03, -1.7869e-02,  2.6396e-02,\n",
              "                      -2.5210e-02, -2.2276e-02, -2.3298e-02,  2.9436e-03,  1.1561e-02,\n",
              "                      -1.8241e-02, -2.4822e-02, -1.1524e-02,  2.4723e-02, -1.0332e-02,\n",
              "                       3.1241e-03])),\n",
              "             ('linear2.weight',\n",
              "              tensor([[-1.7172e-02, -5.0375e-02,  3.9796e-02,  ..., -5.8451e-03,\n",
              "                       -2.0116e-03,  2.4505e-03],\n",
              "                      [ 4.9952e-02, -3.2777e-02, -4.3084e-06,  ..., -1.7434e-02,\n",
              "                        4.2264e-02, -5.4219e-02],\n",
              "                      [-4.4808e-02,  2.4793e-02,  2.5012e-02,  ..., -4.0405e-02,\n",
              "                       -3.7764e-02,  4.2145e-02],\n",
              "                      ...,\n",
              "                      [-4.2247e-02, -2.0703e-02,  1.4359e-02,  ...,  7.3923e-03,\n",
              "                       -2.2167e-02, -4.5863e-02],\n",
              "                      [-4.4202e-02, -3.0057e-03, -4.9013e-02,  ...,  1.0297e-02,\n",
              "                        5.9328e-02,  3.3690e-03],\n",
              "                      [-9.2458e-03,  2.1795e-03,  5.4749e-02,  ..., -1.8542e-02,\n",
              "                        4.8148e-02, -2.3050e-02]])),\n",
              "             ('linear2.bias',\n",
              "              tensor([-0.0010, -0.0363,  0.0553,  0.0008,  0.0282, -0.0443,  0.0161,  0.0751,\n",
              "                       0.0065,  0.0779,  0.0544, -0.0347, -0.0092,  0.0784,  0.0434, -0.0464,\n",
              "                       0.0586, -0.0281,  0.0328,  0.0042,  0.0548, -0.0526,  0.0043, -0.0204,\n",
              "                      -0.0011,  0.0150,  0.0569, -0.0602, -0.0309,  0.0596,  0.0171,  0.0755,\n",
              "                       0.0155,  0.0474, -0.0397,  0.0455,  0.0216,  0.0092, -0.0298,  0.0334,\n",
              "                       0.0613,  0.0618,  0.0553,  0.0016,  0.0538,  0.0175,  0.0538,  0.0115,\n",
              "                      -0.0006, -0.0258,  0.0082, -0.0183,  0.0241, -0.0069, -0.0248, -0.0236,\n",
              "                       0.0357, -0.0322, -0.0568,  0.0145,  0.0406, -0.0327,  0.0216,  0.0279,\n",
              "                      -0.0264,  0.0480, -0.0003, -0.0173,  0.0228,  0.0103,  0.0041,  0.0416,\n",
              "                       0.0619, -0.0296, -0.0028,  0.0037,  0.0115,  0.0228, -0.0223, -0.0160,\n",
              "                      -0.0129, -0.0525,  0.0473,  0.0663, -0.0289, -0.0147, -0.0234, -0.0592,\n",
              "                      -0.0078,  0.0240,  0.0081,  0.0635,  0.0171,  0.0484,  0.0159,  0.0680,\n",
              "                       0.0299,  0.0428, -0.0054, -0.0007,  0.0660,  0.0332, -0.0108, -0.0501,\n",
              "                       0.0454, -0.0075,  0.0575, -0.0469, -0.0178,  0.0361,  0.0130, -0.0320,\n",
              "                      -0.0023, -0.0133, -0.0140, -0.0512,  0.0417,  0.0059, -0.0227,  0.0326,\n",
              "                      -0.0367, -0.0502,  0.0368,  0.0389,  0.0185, -0.0625,  0.0668, -0.0353])),\n",
              "             ('linear3.weight',\n",
              "              tensor([[ 0.0112,  0.0004,  0.1351,  ..., -0.0584, -0.0710, -0.0049],\n",
              "                      [ 0.0382, -0.0142,  0.0658,  ...,  0.0208,  0.1280,  0.0087],\n",
              "                      [ 0.0659,  0.0024,  0.0011,  ...,  0.0644, -0.0120,  0.0409],\n",
              "                      ...,\n",
              "                      [ 0.0601, -0.0449,  0.0078,  ..., -0.0361, -0.1011,  0.0469],\n",
              "                      [ 0.0455, -0.0312,  0.0386,  ...,  0.0216,  0.0340,  0.0550],\n",
              "                      [ 0.0331, -0.0174, -0.0074,  ..., -0.0450,  0.0159,  0.0165]])),\n",
              "             ('linear3.bias',\n",
              "              tensor([-0.0508,  0.0692,  0.0670, -0.0507, -0.0283, -0.0093, -0.0152,  0.0756,\n",
              "                       0.0099, -0.0155]))])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CXI70r7OpI8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}